{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1541530599.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/jt/jg8f8t5d5l94gkq96yn3k41h0000gn/T/ipykernel_44929/1541530599.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GwiHwan-Go/Reproduce_CutMix/blob/main/Training/CutMix.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install pretrainedmodels\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "########## YOUR DIR\n",
    "import sys\n",
    "sys.path.append(\"..\") ## to import parent's folder\n",
    "from Local import DIR \n",
    "########### YOUR DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached https://files.pythonhosted.org/packages/07/02/16044263fe6b291dfdc8bbc8103dbea8d132171bc1121f759e3d0f0e2ee0/torchvision-0.12.0-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: numpy in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: requests in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: torch==1.11.0 in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision) (2.0.12)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.12.0\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 22.1b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"{DIR}/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, csv, img_height, img_width, transform):\n",
    "        self.csv = csv.reset_index()\n",
    "        self.img_ids = csv['image_id'].values\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index]\n",
    "        img = joblib.load(f'{DIR}/train_images/{img_id}.pkl')\n",
    "        img = img.reshape(self.img_height, self.img_width).astype(np.uint8)\n",
    "        #img = 255 - img\n",
    "        img = 255 - img\n",
    "        #img = np.expand_dims(img, axis=2)\n",
    "        \n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.repeat(img, 3, 2)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)['image']\n",
    "        \n",
    "        label_1 = self.csv.iloc[index].grapheme_root\n",
    "        label_2 = self.csv.iloc[index].vowel_diacritic\n",
    "        label_3 = self.csv.iloc[index].consonant_diacritic\n",
    "        \n",
    "        return img, np.array([label_1, label_2, label_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "train_augmentation = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomRotation(20),\n",
    "    ##we can add more augmentation##\n",
    "])\n",
    "\n",
    "valid_augmentation = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_dataset = BengaliDataset(csv=X_train,\n",
    "                            img_height=137,\n",
    "                            img_width=236,\n",
    "                            transform=train_augmentation)\n",
    "valid_dataset = BengaliDataset(csv=X_val,\n",
    "                            img_height=137,\n",
    "                            img_width=236,\n",
    "                            transform=valid_augmentation)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4,\n",
    "                        batch_size=128\n",
    "                       )\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                       shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        batch_size=128\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model, optimizer and loss function - ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretrainedmodels\n",
      "  Using cached https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz\n",
      "Requirement already satisfied: torch in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pretrainedmodels) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pretrainedmodels) (0.12.0)\n",
      "Collecting munch (from pretrainedmodels)\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pretrainedmodels) (4.63.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torch->pretrainedmodels) (4.1.1)\n",
      "Requirement already satisfied: numpy in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (9.1.0)\n",
      "Requirement already satisfied: requests in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (2.27.1)\n",
      "Requirement already satisfied: six in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from munch->pretrainedmodels) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/gwihwango/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (2.0.12)\n",
      "Installing collected packages: munch, pretrainedmodels\n",
      "  Running setup.py install for pretrainedmodels ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed munch-2.5.0 pretrainedmodels-0.7.4\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 22.1b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): None\n",
       "  (last_linear): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pretrainedmodels\n",
    "model_name = 'resnet34'\n",
    "model = pretrainedmodels.__dict__[model_name](pretrained='imagenet')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "## 우리 이미지 사이즈에 맞게 튜닝\n",
    "in_features = model.last_linear.in_features\n",
    "model.last_linear = torch.nn.Linear(in_features, 186) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                      mode='max',\n",
    "                                                      verbose=True,\n",
    "                                                      patience=7,\n",
    "                                                      factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    \n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"  # If you are using CPU\n",
    "for inputs, targets in tqdm(train_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.random.beta(1.0, 1.0) \n",
    "rand_index = torch.randperm(inputs.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_gra = targets[:, 0]\n",
    "targets_vow = targets[:, 1]\n",
    "targets_con = targets[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_targets_gra = targets_gra[rand_index]\n",
    "shuffled_targets_vow = targets_vow[rand_index]\n",
    "shuffled_targets_con = targets_con[rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inputs[5].permute(1, 2, 0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grapheme = logits[:,:168]\n",
    "vowel = logits[:, 168:179]\n",
    "cons = logits[:, 179:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(100):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            logits = model(inputs)\n",
    "            grapheme = logits[:, :168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "\n",
    "            loss = loss_fn(grapheme, targets[:, 0]) + loss_fn(vowel, targets[:, 1]) + loss_fn(cons, targets[:, 2])\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.append(loss.item())\n",
    "        else:\n",
    "            \n",
    "            lam = np.random.beta(1.0, 1.0) \n",
    "            rand_index = torch.randperm(inputs.size()[0])\n",
    "            \n",
    "            targets_gra = targets[:, 0]\n",
    "            targets_vow = targets[:, 1]\n",
    "            targets_con = targets[:, 2]\n",
    "            \n",
    "            shuffled_targets_gra = targets_gra[rand_index]\n",
    "            shuffled_targets_vow = targets_vow[rand_index]\n",
    "            shuffled_targets_con = targets_con[rand_index]\n",
    "            \n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "            \n",
    "            logits = model(inputs)\n",
    "\n",
    "            grapheme = logits[:,:168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "            \n",
    "            loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "            loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "            loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "            \n",
    "            loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    val_loss = []\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(vld_loader):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            logits = model(inputs)\n",
    "\n",
    "            grapheme = logits[:, :168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "\n",
    "            loss = loss_fn(grapheme, targets[:, 0]) + loss_fn(vowel, targets[:, 1]) + loss_fn(cons, targets[:, 2])\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "            vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "            cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "            val_true.append(targets.cpu().numpy())\n",
    "            val_pred.append(np.stack([grapheme, vowel, cons], axis=1))\n",
    "\n",
    "    val_true = np.concatenate(val_true)\n",
    "    val_pred = np.concatenate(val_pred)\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    score_g = recall_score(val_true[:, 0], val_pred[:, 0], average='macro')\n",
    "    score_v = recall_score(val_true[:, 1], val_pred[:, 1], average='macro')\n",
    "    score_c = recall_score(val_true[:, 2], val_pred[:, 2], average='macro')\n",
    "\n",
    "    final_score = np.average([score_g, score_v, score_c], weights=[2, 1, 1])\n",
    "\n",
    "    print(f'train_loss: {train_loss:.5f}; val_loss: {val_loss:.5f}; score: {final_score:.5f}')\n",
    "    print(f'score_g: {score_g:.5f}; score_v: {score_v: .5f}, score_c: {score_c: .5f}')\n",
    "\n",
    "\n",
    "\n",
    "    if final_score > best_score:\n",
    "        best_score = final_score\n",
    "\n",
    "        state_dict = model.cpu().state_dict()\n",
    "        model = model.cuda()\n",
    "        torch.save(state_dict, \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4b10d77f063ea7c690ae79af4b47611cdf22b4ae6c3604143fe18c412509b5a"
  },
  "kernelspec": {
   "display_name": "torch_cv",
   "language": "python",
   "name": "torch_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
