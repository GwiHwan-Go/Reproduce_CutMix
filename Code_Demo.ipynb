{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduce Dataset\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.12.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: torch==1.11.0 in /Users/gwihwango/.pyenv/versions/3.7.3/envs/torch_cv/lib/python3.7/site-packages (from torchvision) (1.11.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.1.1-cp37-cp37m-macosx_10_10_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/gwihwango/.pyenv/versions/3.7.3/envs/torch_cv/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /Users/gwihwango/.pyenv/versions/3.7.3/envs/torch_cv/lib/python3.7/site-packages (from torchvision) (4.2.0)\n",
      "Collecting requests\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.5.18.1-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.2/155.2 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, pillow, idna, charset-normalizer, certifi, requests, torchvision\n",
      "Successfully installed certifi-2022.5.18.1 charset-normalizer-2.0.12 idna-3.3 pillow-9.1.1 requests-2.27.1 torchvision-0.12.0 urllib3-1.26.9\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Users/gwihwango/.pyenv/versions/3.7.3/envs/torch_cv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jt/jg8f8t5d5l94gkq96yn3k41h0000gn/T/ipykernel_29953/759621812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# !pip install torch numpy\n",
    "!pip install torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. cifar-100 \n",
    "##### Transfrom setting as CutMix Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                  std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100('../data', train=True, download=True, transform=transform_train)\n",
    "test_set = torchvision.datasets.CIFAR100('../data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. Tiny-Imagenet\n",
    "##### Transform setting from one of kaggler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip -q tiny-imagenet-200.zip && ls tiny-imagenet-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "\n",
    "        ])\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "\n",
    "        ])\n",
    "\n",
    "def parseClasses(file):\n",
    "    classes = []\n",
    "    filenames = []\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [x.strip() for x in lines]\n",
    "    for x in range(0,len(lines)):\n",
    "        tokens = lines[x].split()\n",
    "        classes.append(tokens[1])\n",
    "        filenames.append(tokens[0])\n",
    "    return filenames,classes\n",
    "    \n",
    "class TImgNetDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and ground truths.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_path, gt_path, class_to_idx=None, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.gt_path = gt_path\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.classidx = []\n",
    "        self.imgs, self.classnames = parseClasses(gt_path)\n",
    "        for classname in self.classnames:\n",
    "            self.classidx.append(self.class_to_idx[classname])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                index (int): Index\n",
    "            Returns:\n",
    "                tuple: (image, y) where y is the label of the image.\n",
    "            \"\"\"\n",
    "            img = None\n",
    "            with open(os.path.join(self.img_path, self.imgs[index]), 'rb') as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert('RGB')\n",
    "                if self.transform is not None:\n",
    "                    img = self.transform(img)\n",
    "            y = self.classidx[index]\n",
    "            return img, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "train_set = datasets.ImageFolder(\"./tiny-imagenet-200/train\", transform=train_transform)\n",
    "\n",
    "val_set = TImgNetDataset(\"./tiny-imagenet-200/val/images\", \"./tiny-imagenet-200/val/val_annotations.txt\", class_to_idx=train_loader.dataset.class_to_idx.copy(),\n",
    "            transform=test_transform)\n",
    "\n",
    "test_set = datasets.ImageFolder(\"./tiny-imagenet-200/test\", transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduce Algorithm\n",
    "----------------------------\n",
    "\n",
    "##### 2-2. Mixup\n",
    "#### 2-3. *CutMix*\n",
    "##### 2-4. Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-1. Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cut(W,H,lam):\n",
    "        \n",
    "    cut_rat = np.sqrt(1. - lam) # define the size of box to cut\n",
    "\n",
    "    cut_w = np.int64(W * cut_rat) # define the width of box to cut\n",
    "    cut_h = np.int64(H * cut_rat) \n",
    "\n",
    "    \n",
    "    cx = np.random.randint(W) # uniform distribution\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W) # Cut, return coordinates of the box \n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(valid_loader))\n",
    "\n",
    "rand_index = torch.randperm(images.shape[0]) #Make random indices in a batch to mix between a batch\n",
    "shuffled_images = images[rand_index]\n",
    "\n",
    "lam = np.random.beta(1.0, 1.0) # Beta distributtion\n",
    "\n",
    "cutmix_images = images.clone().detach() # copy Images\n",
    "cutout_images = images.clone().detach() \n",
    "mixup_images = images.clone().detach() \n",
    "\n",
    "bbx1, bby1, bbx2, bby2 = cut(images.shape[2], images.shape[3], lam) # define a box to cut and mix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CUTMIX ##\n",
    "cutmix_images[:, :, bbx1:bbx2, bby1:bby2] = cutmix_images[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "##* CUTOUT *##\n",
    "cutout_images[:, :, bbx1:bbx2, bby1:bby2] = 0.\n",
    "\n",
    "## MIXUP ##\n",
    "mixup_images = lam * mixup_images + (1 - lam) * mixup_images[rand_index,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def show_cutMixed_compare(row1, row2, row3, row4, nums_show): # Let's compare each Algorithm in visual.\n",
    "\n",
    "    rand_index = torch.randperm(row1.size()[0])[:nums_show]\n",
    "    to_compare = torch.cat([row1[rand_index,:,:],row2[rand_index,:,:],row3[rand_index,:,:],row4[rand_index,:,:]],dim=0)\n",
    "    plt.figure(figsize=(50,50))\n",
    "    grid = torchvision.utils.make_grid(tensor=to_compare, nrow=nums_show)\n",
    "    plt.ylabel(\"Cutout | Mixup | CutMix | Image\",fontsize=165, rotation = 90)\n",
    "    plt.title(\"Comparison between augmentations\", fontsize=100)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "\n",
    "show_cutMixed_compare(images, cutmix_images, mixup_images, cutout_images, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduce Models\n",
    "----------------------------\n",
    "##### 3-1. ResNet-50\n",
    "##### 3-2. PyramidNet-110 & 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from torchvision import models\n",
    "\n",
    "# resnet Model Loading\n",
    "model = models.resnet18(pretrained=False) \n",
    "\n",
    "## Tunning the final output as our number of labels.\n",
    "model.fc = torch.nn.Linear(512, 200) ## if resnet50 : 2048,200\n",
    "print('the number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    outchannel_ratio = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)        \n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = torch.autograd.Variable(torch.cuda.FloatTensor(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1]).fill_(0)) \n",
    "            out += torch.cat((shortcut, padding), 1)\n",
    "        else:\n",
    "            out += shortcut \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    outchannel_ratio = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, (planes), kernel_size=3, stride=stride, padding=1, bias=False, groups=1)\n",
    "        self.bn3 = nn.BatchNorm2d((planes))\n",
    "        self.conv3 = nn.Conv2d((planes), planes * Bottleneck.outchannel_ratio, kernel_size=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(planes * Bottleneck.outchannel_ratio)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    " \n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out = self.bn4(out)\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = torch.autograd.Variable(torch.cuda.FloatTensor(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1]).fill_(0)) \n",
    "            out += torch.cat((shortcut, padding), 1)\n",
    "        else:\n",
    "            out += shortcut \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PyramidNet(nn.Module):\n",
    "        \n",
    "    def __init__(self, dataset, depth, alpha, num_classes, bottleneck=False):\n",
    "        super(PyramidNet, self).__init__()   \t\n",
    "        self.dataset = dataset\n",
    "        if self.dataset.startswith('cifar'):\n",
    "            self.inplanes = 16\n",
    "            if bottleneck == True:\n",
    "                n = int((depth - 2) / 9)\n",
    "                block = Bottleneck\n",
    "            else:\n",
    "                n = int((depth - 2) / 6)\n",
    "                block = BasicBlock\n",
    "\n",
    "            self.addrate = alpha / (3*n*1.0)\n",
    "\n",
    "            self.input_featuremap_dim = self.inplanes\n",
    "            self.conv1 = nn.Conv2d(3, self.input_featuremap_dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.input_featuremap_dim)\n",
    "\n",
    "            self.featuremap_dim = self.input_featuremap_dim \n",
    "            self.layer1 = self.pyramidal_make_layer(block, n)\n",
    "            self.layer2 = self.pyramidal_make_layer(block, n, stride=2)\n",
    "            self.layer3 = self.pyramidal_make_layer(block, n, stride=2)\n",
    "\n",
    "            self.final_featuremap_dim = self.input_featuremap_dim\n",
    "            self.bn_final= nn.BatchNorm2d(self.final_featuremap_dim)\n",
    "            self.relu_final = nn.ReLU(inplace=True)\n",
    "            self.avgpool = nn.AvgPool2d(8)\n",
    "            self.fc = nn.Linear(self.final_featuremap_dim, num_classes)\n",
    "\n",
    "        elif dataset == 'imagenet':\n",
    "            blocks ={18: BasicBlock, 34: BasicBlock, 50: Bottleneck, 101: Bottleneck, 152: Bottleneck, 200: Bottleneck}\n",
    "            layers ={18: [2, 2, 2, 2], 34: [3, 4, 6, 3], 50: [3, 4, 6, 3], 101: [3, 4, 23, 3], 152: [3, 8, 36, 3], 200: [3, 24, 36, 3]}\n",
    "\n",
    "            if layers.get(depth) is None:\n",
    "                if bottleneck == True:\n",
    "                    blocks[depth] = Bottleneck\n",
    "                    temp_cfg = int((depth-2)/12)\n",
    "                else:\n",
    "                    blocks[depth] = BasicBlock\n",
    "                    temp_cfg = int((depth-2)/8)\n",
    "\n",
    "                layers[depth]= [temp_cfg, temp_cfg, temp_cfg, temp_cfg]\n",
    "                print('=> the layer configuration for each stage is set to', layers[depth])\n",
    "\n",
    "            self.inplanes = 64            \n",
    "            self.addrate = alpha / (sum(layers[depth])*1.0)\n",
    "\n",
    "            self.input_featuremap_dim = self.inplanes\n",
    "            self.conv1 = nn.Conv2d(3, self.input_featuremap_dim, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.input_featuremap_dim)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "            self.featuremap_dim = self.input_featuremap_dim \n",
    "            self.layer1 = self.pyramidal_make_layer(blocks[depth], layers[depth][0])\n",
    "            self.layer2 = self.pyramidal_make_layer(blocks[depth], layers[depth][1], stride=2)\n",
    "            self.layer3 = self.pyramidal_make_layer(blocks[depth], layers[depth][2], stride=2)\n",
    "            self.layer4 = self.pyramidal_make_layer(blocks[depth], layers[depth][3], stride=2)\n",
    "\n",
    "            self.final_featuremap_dim = self.input_featuremap_dim\n",
    "            self.bn_final= nn.BatchNorm2d(self.final_featuremap_dim)\n",
    "            self.relu_final = nn.ReLU(inplace=True)\n",
    "            self.avgpool = nn.AvgPool2d(7) \n",
    "            self.fc = nn.Linear(self.final_featuremap_dim, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def pyramidal_make_layer(self, block, block_depth, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1: # or self.inplanes != int(round(featuremap_dim_1st)) * block.outchannel_ratio:\n",
    "            downsample = nn.AvgPool2d((2,2), stride = (2, 2), ceil_mode=True)\n",
    "\n",
    "        layers = []\n",
    "        self.featuremap_dim = self.featuremap_dim + self.addrate\n",
    "        layers.append(block(self.input_featuremap_dim, int(round(self.featuremap_dim)), stride, downsample))\n",
    "        for i in range(1, block_depth):\n",
    "            temp_featuremap_dim = self.featuremap_dim + self.addrate\n",
    "            layers.append(block(int(round(self.featuremap_dim)) * block.outchannel_ratio, int(round(temp_featuremap_dim)), 1))\n",
    "            self.featuremap_dim  = temp_featuremap_dim\n",
    "        self.input_featuremap_dim = int(round(self.featuremap_dim)) * block.outchannel_ratio\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "\n",
    "            x = self.bn_final(x)\n",
    "            x = self.relu_final(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        elif self.dataset == 'imagenet':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "\n",
    "            x = self.bn_final(x)\n",
    "            x = self.relu_final(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "dataset='cifar100'\n",
    "depth=110\n",
    "alpha=64\n",
    "NUM_CLASSES=100\n",
    "use_bottleneck=False\n",
    "\n",
    "model = PyramidNet(dataset, depth, alpha, NUM_CLASSES,\n",
    "                        use_bottleneck)\n",
    "print('the number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Introduce Training Process\n",
    "----------------------------\n",
    "##### 4-1. Hyper-Params\n",
    "##### 4-2. Loss functions per every methods\n",
    "##### 4-3. Savepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "    \n",
    "def accuracy(output, target, topk=(1,)): ## Calculate top-1-errors and top-5-errors\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "        wrong_k = batch_size - correct_k\n",
    "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "    \n",
    "def train_model(model, \n",
    "                train, \n",
    "                valid,\n",
    "                resume = None,\n",
    "                n_iters=300, \n",
    "                learn_rate=0.1, \n",
    "                weight_decay=0, \n",
    "                which_method=0\n",
    "                ):  # Lists to store model's performance information\n",
    "  po = 0\n",
    "  top1_errs, top5_errs, train_losses = [], [], []\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learn_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  print(f\"we are going to use {device}\")\n",
    "###############Checkpoint Zone################\n",
    "  if resume is not None :\n",
    "    checkpoint = torch.load(resume) \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss = checkpoint['loss']\n",
    "    num_epochs = checkpoint['total_epochs'] - checkpoint['epoch']\n",
    "    top1_errs = checkpoint['top1_err']\n",
    "    top5_errs = checkpoint['top5_err']\n",
    "  else :\n",
    "    num_epochs = n_iters\n",
    "###############Checkpoint Zone###############\n",
    "  \n",
    "  print(f\"We still have to go {num_epochs} epochs\" )\n",
    "  for i in trange(num_epochs):\n",
    "\n",
    "    ##############################learning_decay#####################################\n",
    "    if (num_epochs-i) < 75 :\n",
    "      newpo = 3\n",
    "    elif (num_epochs-i) < 150 :\n",
    "      newpo = 2\n",
    "    elif (num_epochs-i) < 225 :\n",
    "      newpo = 1\n",
    "    else :\n",
    "      newpo = 0\n",
    "    if po != newpo :\n",
    "      po = newpo\n",
    "      lr = learn_rate * (0.1 ** po) ##learning_decay\n",
    "      print(f\"lr has decayed to {lr}\")\n",
    "      optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    ##############################learning_decay#####################################\n",
    "\n",
    "    try :\n",
    "      for images, labels in tqdm(train) :\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        model = model.to(device)\n",
    "        model.train() # For Dropout and Batch Norm layers #\n",
    "\n",
    "        if which_method and (np.random.rand()<0.5) :\n",
    "\n",
    "          lam = np.random.beta(1.0, 1.0)\n",
    "          rand_index = torch.randperm(images.size()[0])\n",
    "          shuffled_labels = labels[rand_index]\n",
    "\n",
    "          ##############Do Something On Data###############\n",
    "          if which_method == 1 : ##cutmix\n",
    "            bbx1, bby1, bbx2, bby2 = cut(images.shape[2], images.shape[3], lam) # define a box to cut and mix\n",
    "            images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2] #cut and mix\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.shape[-1] * images.shape[-2]))\n",
    "          if which_method == 2 : ##mixup\n",
    "            images = lam * images + (1 - lam) * images[rand_index,:,:]\n",
    "          if which_method == 3 : ##cutout\n",
    "            bbx1, bby1, bbx2, bby2 = cut(images.shape[2], images.shape[3], lam) # define a box to cut and mix\n",
    "            images[:, :, bbx1:bbx2, bby1:bby2] = 0.\n",
    "          ##############Do Something On Data###############\n",
    "\n",
    "          #############Forward Pass##############\n",
    "          out = model(images) \n",
    "          #############Forward Pass##############\n",
    "\n",
    "          ##############Modify Loss Function###############\n",
    "          if which_method == 1 : ##cutmix\n",
    "            loss = criterion(out, labels) * lam + criterion(out, shuffled_labels)*(1.0-lam) # compute the total loss\n",
    "          if which_method == 2 : ##mixup\n",
    "            loss = criterion(out, labels) * lam + criterion(out, shuffled_labels)*(1.0-lam)\n",
    "          if which_method == 3 : ##cutout\n",
    "            loss = criterion(out, labels)\n",
    "          ##############Modify Loss Function###############\n",
    "\n",
    "        else : ## if random.rand > 0.5 or which_method == 0\n",
    "          \n",
    "          out = model(images)\n",
    "          loss = criterion(out, labels)\n",
    "\n",
    "        loss.backward()               # backward pass (compute parameter updates)\n",
    "        optimizer.step()              # make the updates for each parameter\n",
    "        optimizer.zero_grad()         # reset the gradients for the next iteration\n",
    "      \n",
    "      #####################model Evaluation#############\n",
    "      sum_top1, sum_top5 = 0,0\n",
    "      \n",
    "                \n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for images, labels in valid :\n",
    "\n",
    "          images,labels = images.to(device), labels.to(device)\n",
    "          output = model(images)\n",
    "          \n",
    "          top1, top5 = accuracy(output, labels, topk=(1,5))\n",
    "          sum_top1+=top1.item()\n",
    "          sum_top5+=top5.item()\n",
    "      \n",
    "      size = len(valid)\n",
    "      #####################model Evaluation#############\n",
    "\n",
    "      ###################save history#####################\n",
    "      train_losses.append(loss.item())\n",
    "      top1_errs.append(sum_top1/size)\n",
    "      top5_errs.append(sum_top5/size)\n",
    "      ###################save history#####################\n",
    "\n",
    "      if (i+1) % 10 == 0 :\n",
    "\n",
    "        PATH=f\"./{300-(num_epochs-i)}th_checkpoint.pt\"\n",
    "        torch.save({'total_epochs':num_epochs, 'epoch': i, 'model_state_dict': model.state_dict(), \n",
    "        'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, 'top1_err': top1_errs, 'top5_err': top5_errs, 'train_loss':train_losses }, PATH)\n",
    "        print(f\"Successfully saved untill {i} step in {PATH} with model, optimizer, and loss.\")\n",
    "\n",
    "    except :\n",
    "      PATH=f\"./{300-(num_epochs-i)}th_checkpoint.pt\"\n",
    "      torch.save({'total_epochs':num_epochs, 'epoch': i, 'model_state_dict': model.state_dict(), \n",
    "      'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, 'top1_err': top1_errs, 'top5_err': top5_errs , 'train_loss':train_losses}, PATH)\n",
    "      print(f\"Successfully saved untill {i} step in {PATH} with model, optimizer, and loss.\")\n",
    "      return [top1_errs,top5_errs]\n",
    "\n",
    "  PATH = \"./trained.pt\"\n",
    "  print(\"train finished\")\n",
    "  print(f\"Successfully saved untill {n_iters} step in {PATH} with model, optimizer, and loss.\")\n",
    "  torch.save({'total_epochs':num_epochs, 'epoch': i, 'model_state_dict': model.state_dict(), \n",
    "  'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, 'top1_err': top1_errs, 'top5_err': top5_errs, 'train_loss':train_losses}, PATH)\n",
    "  return [top1_errs,top5_errs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated()/1024**2)\n",
    "logs = train_model(model, train_loader, valid_loader, n_iters=300, \n",
    "                    learn_rate=0.1, \n",
    "                    weight_decay=0, which_method=1) \n",
    "\n",
    "#which method \n",
    "# 0: nothing, \n",
    "# 1: cutmix, \n",
    "# 2: mixup \n",
    "# 3: cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"./32th_checkpoint.pt\" ## Where checkpoint saved. ## saved checkpoint\n",
    "\n",
    "train_model(model,\n",
    "            train_loader, \n",
    "            valid_loader,\n",
    "            n_iters=300, \n",
    "            learn_rate=0.1, \n",
    "            resume=PATH,\n",
    "            weight_decay=0, \n",
    "            which_method=1) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eaa0f91b0726c821666ba3ce98c266219a5988e541e89248709bd3017fde90d2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('torch_cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
